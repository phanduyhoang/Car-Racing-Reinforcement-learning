{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_baselines3[extra]\n",
        "\n",
        "!apt-get install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install gym\n",
        "!pip install pygame\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcPWLpxt6QgX",
        "outputId": "4e7385c5-83b6-4feb-bbdf-3d9a53bb20c0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3[extra]\n",
            "  Downloading stable_baselines3-2.4.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from stable_baselines3[extra])\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (4.10.0.84)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (2.17.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (13.9.4)\n",
            "Collecting ale-py>=0.9.0 (from stable_baselines3[extra])\n",
            "  Downloading ale_py-0.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable_baselines3[extra]) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable_baselines3[extra]) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<1.1.0,>=0.29.1->stable_baselines3[extra])\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.69.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (4.25.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable_baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable_baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable_baselines3[extra]) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->stable_baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable_baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable_baselines3[extra]) (2024.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable_baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable_baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable_baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable_baselines3[extra]) (3.0.2)\n",
            "Downloading ale_py-0.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.4.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.0/184.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, ale-py, stable_baselines3\n",
            "Successfully installed ale-py-0.10.1 farama-notifications-0.0.4 gymnasium-1.0.0 stable_baselines3-2.4.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  swig4.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig4.0-examples swig4.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig4.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,116 kB of archives.\n",
            "After this operation, 5,542 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n",
            "Fetched 1,116 kB in 2s (481 kB/s)\n",
            "Selecting previously unselected package swig4.0.\n",
            "(Reading database ... 124565 files and directories currently installed.)\n",
            "Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n",
            "Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n",
            "Unpacking swig (4.0.2-1ubuntu1) ...\n",
            "Setting up swig4.0 (4.0.2-1ubuntu1) ...\n",
            "Setting up swig (4.0.2-1ubuntu1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Collecting swig==4.* (from gymnasium[box2d])\n",
            "  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp311-cp311-linux_x86_64.whl size=2351177 sha256=db32dcb1cf80a4c4544423ca1b403f84a25c76881f5c3c30c02d37c66558afcf\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/f1/0c/d56f4a2bdd12bae0a0693ec33f2f0daadb5eb9753c78fa5308\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: swig, box2d-py\n",
            "Successfully installed box2d-py-2.3.5 swig-4.3.0\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (2.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import os"
      ],
      "metadata": {
        "id": "FqSfTWmo5g8z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env=gym.make('CarRacing-v3')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki1XzB766bFz",
        "outputId": "e62b6cc1-02f0-459c-c34d-07a9f3e0ae9f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from collections import deque\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F0YR1nLC7Zfb",
        "outputId": "12935207-ad35-465d-b4cb-a0777ad666d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shimmy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztMjNfTX9QTj",
        "outputId": "5e87eaa9-f9cb-41d1-97de-3ea3cae8e01e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.11/dist-packages (from shimmy) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "NUM_ENVS=4\n",
        "env=DummyVecEnv([lambda: gym.make('CarRacing-v3') for _ in range(NUM_ENVS)])\n",
        "\n",
        "print(f'using {device}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2UStnjP8nt1",
        "outputId": "3ae97706-84b2-4145-a92c-fa57e4c87790"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_size=env.observation_space.shape\n",
        "state_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvzE8PpW9eTf",
        "outputId": "1493a1f0-02a3-4a12-b769-dcc630e98e04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96, 96, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "action_size=env.action_space\n",
        "action_size\n",
        "\n",
        "\n",
        "# Print action space details\n",
        "print(\"Action Space:\", env.action_space)\n",
        "print(\"Action Space Shape:\", env.action_space.shape)\n",
        "print(\"Action Space High (Max):\", env.action_space.high)\n",
        "print(\"Action Space Low (Min):\", env.action_space.low)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWW_UIA8ojN8",
        "outputId": "9fd66c76-30bb-4351-affa-381893168c21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action Space: Box([-1.  0.  0.], 1.0, (3,), float32)\n",
            "Action Space Shape: (3,)\n",
            "Action Space High (Max): [1. 1. 1.]\n",
            "Action Space Low (Min): [-1.  0.  0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = env.reset()\n",
        "print(f\"Type of result: {type(result)}, Result: {result}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chs6HIyi4HFc",
        "outputId": "0b5ebe51-95b0-43b5-a33c-3c6f17fa7e33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of result: <class 'numpy.ndarray'>, Result: [[[[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]]\n",
            "\n",
            "\n",
            " [[[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]]\n",
            "\n",
            "\n",
            " [[[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]]\n",
            "\n",
            "\n",
            " [[[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]\n",
            "\n",
            "  [[0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   ...\n",
            "   [0 0 0]\n",
            "   [0 0 0]\n",
            "   [0 0 0]]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "LEARNING_RATE = 0.001\n",
        "DISCOUNT = 0.99\n",
        "EPSILON = 1.0\n",
        "EPSILON_DECAY = 0.995\n",
        "EPSILON_MIN = 0.05\n",
        "MEMORY_SIZE = 100000\n",
        "BATCH_SIZE = 64\n",
        "TARGET_UPDATE_FREQ = 1000\n",
        "TOTAL_TIMESTEPS = 500000\n",
        "TRAINING_START = 10000\n",
        "\n",
        "class ReplayBuffer:\n",
        "  def __init__(self,size):\n",
        "    self.buffer=deque(maxlen=size)\n",
        "  def add(self,experience):\n",
        "    self.buffer.append(experience)\n",
        "  def sample(self, batch_size):\n",
        "    batch=random.sample(self.buffer, batch_size)\n",
        "    states, actions,rewards,next_states,dones=zip(*batch)\n",
        "    return (\n",
        "        torch.tensor(np.array(states), dtype=torch.float32, device=device),\n",
        "        torch.tensor(np.array(actions),dtype=torch.float32,device=device),\n",
        "        torch.tensor(rewards, dtype=torch.float32, device=device),\n",
        "        torch.tensor(np.array(next_states), dtype=torch.float32, device=device),\n",
        "        torch.tensor(np.array(dones), dtype=torch.float32, device=device),\n",
        "    )\n",
        "  def can_sample(self, batch_size):\n",
        "    return len(self.buffer)>=batch_size\n",
        "\n",
        "memory= ReplayBuffer(MEMORY_SIZE)\n",
        "\n",
        "\n",
        "#Q-network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # Convolutional layers\n",
        "            #output size=(Input Size-Kernel Size+2xPadding)/Stride + 1\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=2, padding=2), # Output: 48x48x32\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1), # Output: 24x24x64\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1), # Output: 12x12x128\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "\n",
        "\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1), # Output: 6x6x128\n",
        "\n",
        "            # Flattening the output for fully connected layers\n",
        "            nn.Flatten(),\n",
        "\n",
        "            # Fully connected layers\n",
        "            nn.Linear(6 * 6 * 128, 512),  # Flattened output size is 6x6x128\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 3)  # Output layer for Q-values\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "      x=self.network(x)\n",
        "      steering=torch.tanh(x[:,0])\n",
        "      accceleartion=torch.sigmoid(x[:,1])\n",
        "      brake=torch.sigmoid(x[:,2])\n",
        "      return torch.stack((steering,accceleartion,brake), dim=1)\n",
        "\n",
        "model=DQN(state_size,action_size).to(device)\n",
        "target_model=DQN(state_size, action_size).to(device)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "target_model.eval()\n",
        "\n",
        "\n",
        "optimizer=optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn=nn.MSELoss()\n",
        "\n",
        "def train():\n",
        "  if not memory.can_sample(BATCH_SIZE):\n",
        "    return\n",
        "\n",
        "  states, actions, rewards, next_states,dones=memory.sample(BATCH_SIZE)\n",
        "  with torch.no_grad():\n",
        "    target_qs=target_model(next_states)\n",
        "    max_future_q=torch.max(target_qs, dim=1)[0]\n",
        "    new_qs=rewards+DISCOUNT*max_future_q\n",
        "  current_qs=model(states)\n",
        "  #current_qs=torch.gather(1, actions.unsqueeze(1)).squeeze(1)#need to fix this here since our action space is continuous(maybe just remove this?)\n",
        "\n",
        "# Calculate losses for each action component\n",
        "  loss_steering = loss_fn(current_qs[:, 0], new_qs[:, 0])  # Loss for steering\n",
        "  loss_acceleration = loss_fn(current_qs[:, 1], new_qs[:, 1])  # Loss for acceleration\n",
        "  loss_brake = loss_fn(current_qs[:, 2], new_qs[:, 2])  # Loss for braking\n",
        "\n",
        "  # Total loss is the sum of individual losses\n",
        "  loss = loss_steering + loss_acceleration + loss_brake\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss.item()\n",
        "\n",
        "state = env.reset()  # Shape: (NUM_ENVS, H, W, C)\n",
        "state = torch.tensor(state.transpose((0, 3, 1, 2)), dtype=torch.float32, device=device) / 255.0\n",
        "\n",
        "\n",
        "\n",
        "step=0\n",
        "episode_rewards=[]\n",
        "episode_lengths=[]\n",
        "current_episode_reward=np.zeros(NUM_ENVS)\n",
        "current_episode_length=np.zeros(NUM_ENVS)\n",
        "\n",
        "while step<TOTAL_TIMESTEPS:\n",
        "  if np.random.rand() < EPSILON:\n",
        "    action=np.array([[np.random.uniform(-1,1),np.random.uniform(0,1),np.random.uniform(0,1)] for _ in range(NUM_ENVS)])\n",
        "  else:\n",
        "    with torch.no_grad():\n",
        "      action=model(state).cpu().numpy()\n",
        "  next_state,reward, done,_=env.step(action)\n",
        "  next_state = torch.tensor(next_state.transpose((0, 3, 1, 2)), dtype=torch.float32, device=device) / 255.0\n",
        "  for i in range(NUM_ENVS):\n",
        "    memory.add((state[i].cpu().numpy(), action[i], reward[i], next_state[i].cpu().numpy(), done[i]))\n",
        "    current_episode_reward[i]+=reward[i]\n",
        "    current_episode_length[i]+=1\n",
        "    if done[i]:\n",
        "      episode_rewards.append(current_episode_reward[i])\n",
        "      episode_lengths.append(current_episode_length[i])\n",
        "      current_episode_reward[i]=0\n",
        "      current_episode_length[i]=0\n",
        "  state=next_state\n",
        "  step+=NUM_ENVS\n",
        "      # Train only after collecting enough data\n",
        "  loss = None\n",
        "  if step > TRAINING_START and step % 4 == 0:\n",
        "      loss = train()\n",
        "  if step % TARGET_UPDATE_FREQ==0:\n",
        "    target_model.load_state_dict(model.state_dict())\n",
        "  if EPSILON>EPSILON_MIN:\n",
        "    EPSILON*=EPSILON_DECAY\n",
        "  if step % 10000==0:\n",
        "    avg_reward=np.mean(episode_rewards[-100:] if len(episode_rewards)>0 else 0)\n",
        "    avg_length = np.mean(episode_lengths[-100:]) if len(episode_lengths) > 0 else 0\n",
        "    last_loss = loss if loss is not None else 0\n",
        "\n",
        "torch.save(model.state_dict(), \"dqn_halminton_pytorch.pth\")\n"
      ],
      "metadata": {
        "id": "hn6TLAkK9sNt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "6bc6e586-d335-4f03-e8b2-430b4e4e0d06"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "too many indices for tensor of dimension 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-52e6b6d4086e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    132\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mTRAINING_START\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mTARGET_UPDATE_FREQ\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-52e6b6d4086e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Calculate losses for each action component\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m   \u001b[0mloss_steering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Loss for steering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m   \u001b[0mloss_acceleration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Loss for acceleration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0mloss_brake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_qs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Loss for braking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Deep Q network method\n",
        "#IMPORTANT: in order to use Deep Q network method we need to  Discretize\n",
        "#a continuous action space to use with DQN. this can be computationally expensive\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import itertools\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.001\n",
        "DISCOUNT = 0.99\n",
        "EPSILON = 1.0\n",
        "EPSILON_DECAY = 0.995\n",
        "EPSILON_MIN = 0.05\n",
        "MEMORY_SIZE = 100000\n",
        "BATCH_SIZE = 64\n",
        "TARGET_UPDATE_FREQ = 1000\n",
        "TOTAL_TIMESTEPS = 500000\n",
        "TRAINING_START = 10000\n",
        "\n",
        "# Action space discretization\n",
        "NUM_BINS = 20  # Consider reducing to 10 for computational feasibility\n",
        "STEERING_MIN, STEERING_MAX = -1.0, 1.0  # Define based on your environment\n",
        "ACCELERATION_MIN, ACCELERATION_MAX = 0.0, 1.0\n",
        "BRAKE_MIN, BRAKE_MAX = 0.0, 1.0\n",
        "\n",
        "# Generate bins for each action component\n",
        "steering_bins = np.linspace(STEERING_MIN, STEERING_MAX, NUM_BINS)\n",
        "acceleration_bins = np.linspace(ACCELERATION_MIN, ACCELERATION_MAX, NUM_BINS)\n",
        "brake_bins = np.linspace(BRAKE_MIN, BRAKE_MAX, NUM_BINS)\n",
        "\n",
        "# Create all possible action combinations\n",
        "action_combinations = list(itertools.product(range(NUM_BINS), repeat=3))  # (steering_bin, acceleration_bin, brake_bin)\n",
        "NUM_ACTIONS = len(action_combinations)  # 20^3 = 8000\n",
        "\n",
        "# Precompute action vectors for each discrete action\n",
        "def get_action_vector(action_index):\n",
        "    steering_idx, acceleration_idx, brake_idx = action_combinations[action_index]\n",
        "    steering = steering_bins[steering_idx]\n",
        "    acceleration = acceleration_bins[acceleration_idx]\n",
        "    brake = brake_bins[brake_idx]\n",
        "    return np.array([steering, acceleration, brake])\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.tensor(np.array(states), dtype=torch.float32, device=device),\n",
        "            torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1),  # Actions as indices\n",
        "            torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1),\n",
        "            torch.tensor(np.array(next_states), dtype=torch.float32, device=device),\n",
        "            torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1),\n",
        "        )\n",
        "\n",
        "    def can_sample(self, batch_size):\n",
        "        return len(self.buffer) >= batch_size\n",
        "\n",
        "memory = ReplayBuffer(MEMORY_SIZE)\n",
        "\n",
        "# Q-network\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            # Convolutional layers\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, stride=2, padding=2),  # Output: (32, 24, 24)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2, padding=1),  # Output: (64, 12, 12)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1),  # Output: (128, 6, 6)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),  # Output: (128, 3, 3)\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3 * 3 * 128, 512),  # Updated from 6*6*128 to 3*3*128\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_size)  # Output layer for Q-values\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)  # Shape: (batch_size, NUM_ACTIONS)\n",
        "\n",
        "# Initialize networks\n",
        "state_size = (3, 48, 48)  # Example state size, adjust based on your environment\n",
        "action_size = NUM_ACTIONS\n",
        "\n",
        "model = DQN(state_size, action_size).to(device)\n",
        "target_model = DQN(state_size, action_size).to(device)\n",
        "target_model.load_state_dict(model.state_dict())\n",
        "target_model.eval()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# Training function\n",
        "def train_step():\n",
        "    if not memory.can_sample(BATCH_SIZE):\n",
        "        return None\n",
        "\n",
        "    states, actions, rewards, next_states, dones = memory.sample(BATCH_SIZE)\n",
        "\n",
        "    # Current Q-values\n",
        "    current_qs = model(states).gather(1, actions)  # Shape: (batch_size, 1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Next Q-values from target network\n",
        "        next_qs = target_model(next_states)\n",
        "        max_next_qs, _ = torch.max(next_qs, dim=1, keepdim=True)  # Shape: (batch_size, 1)\n",
        "        target_qs = rewards + DISCOUNT * max_next_qs * (1 - dones)  # Shape: (batch_size, 1)\n",
        "\n",
        "    # Compute loss\n",
        "    loss = loss_fn(current_qs, target_qs)\n",
        "\n",
        "    # Optimize the model\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "# Initialize environment (replace with your actual environment)\n",
        "class DummyEnv:\n",
        "    def __init__(self, num_envs, state_shape):\n",
        "        self.num_envs = num_envs\n",
        "        self.state_shape = state_shape\n",
        "\n",
        "    def reset(self):\n",
        "        return np.random.randint(0, 255, size=(self.num_envs, *self.state_shape)).astype(np.uint8)\n",
        "\n",
        "    def step(self, actions):\n",
        "        next_state = np.random.randint(0, 255, size=(self.num_envs, *self.state_shape)).astype(np.uint8)\n",
        "        reward = np.random.rand(self.num_envs)\n",
        "        done = np.random.choice([False, True], size=self.num_envs, p=[0.95, 0.05])\n",
        "        info = {}\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "# Example parameters, adjust based on your environment\n",
        "NUM_ENVS = 4\n",
        "env = DummyEnv(NUM_ENVS, (48, 48, 3))\n",
        "\n",
        "# Initialize state\n",
        "state = env.reset()  # Shape: (NUM_ENVS, H, W, C)\n",
        "state = torch.tensor(state.transpose((0, 3, 1, 2)), dtype=torch.float32, device=device) / 255.0\n",
        "\n",
        "step = 0\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "current_episode_reward = np.zeros(NUM_ENVS)\n",
        "current_episode_length = np.zeros(NUM_ENVS)\n",
        "\n",
        "while step < TOTAL_TIMESTEPS:\n",
        "    if np.random.rand() < EPSILON:\n",
        "        # Select random discrete action indices\n",
        "        action_indices = np.random.randint(0, NUM_ACTIONS, size=NUM_ENVS)\n",
        "        action_vectors = np.array([get_action_vector(idx) for idx in action_indices])\n",
        "    else:\n",
        "        with torch.no_grad():\n",
        "            qs = model(state)  # Shape: (NUM_ENVS, NUM_ACTIONS)\n",
        "            action_indices = qs.argmax(dim=1).cpu().numpy()\n",
        "            action_vectors = np.array([get_action_vector(idx) for idx in action_indices])\n",
        "\n",
        "    # Execute actions in the environment\n",
        "    next_state, reward, done, _ = env.step(action_vectors)\n",
        "    next_state = torch.tensor(next_state.transpose((0, 3, 1, 2)), dtype=torch.float32, device=device) / 255.0\n",
        "\n",
        "    for i in range(NUM_ENVS):\n",
        "        memory.add((\n",
        "            state[i].cpu().numpy(),\n",
        "            action_indices[i],\n",
        "            reward[i],\n",
        "            next_state[i].cpu().numpy(),\n",
        "            done[i]\n",
        "        ))\n",
        "        current_episode_reward[i] += reward[i]\n",
        "        current_episode_length[i] += 1\n",
        "        if done[i]:\n",
        "            episode_rewards.append(current_episode_reward[i])\n",
        "            episode_lengths.append(current_episode_length[i])\n",
        "            current_episode_reward[i] = 0\n",
        "            current_episode_length[i] = 0\n",
        "\n",
        "    state = next_state\n",
        "    step += NUM_ENVS\n",
        "\n",
        "    # Train only after collecting enough data\n",
        "    loss = None\n",
        "    if step > TRAINING_START and step % 4 == 0:\n",
        "        loss = train_step()\n",
        "\n",
        "    # Update target network\n",
        "    if step % TARGET_UPDATE_FREQ == 0:\n",
        "        target_model.load_state_dict(model.state_dict())\n",
        "\n",
        "    # Decay epsilon\n",
        "    if EPSILON > EPSILON_MIN:\n",
        "        EPSILON *= EPSILON_DECAY\n",
        "        EPSILON = max(EPSILON, EPSILON_MIN)\n",
        "\n",
        "    # Logging\n",
        "    if step % 10000 == 0:\n",
        "        if len(episode_rewards) >= 100:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            avg_length = np.mean(episode_lengths[-100:])\n",
        "        else:\n",
        "            avg_reward = np.mean(episode_rewards) if len(episode_rewards) > 0 else 0\n",
        "            avg_length = np.mean(episode_lengths) if len(episode_lengths) > 0 else 0\n",
        "        last_loss = loss if loss is not None else 0\n",
        "        print(f\"Step: {step}, Avg Reward: {avg_reward:.2f}, Avg Length: {avg_length:.2f}, Loss: {last_loss:.4f}, Epsilon: {EPSILON:.4f}\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"dqn_halminton_pytorch_discrete.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ldhO01ZzEeN4",
        "outputId": "bcbf1e13-fb18-4caf-aa3d-aefba1561f3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 10000, Avg Reward: 8.71, Avg Length: 17.42, Loss: 0.0000, Epsilon: 0.0500\n",
            "Step: 20000, Avg Reward: 9.61, Avg Length: 19.50, Loss: 194.5361, Epsilon: 0.0500\n",
            "Step: 30000, Avg Reward: 12.33, Avg Length: 24.42, Loss: 15576.6113, Epsilon: 0.0500\n",
            "Step: 40000, Avg Reward: 11.57, Avg Length: 23.16, Loss: 116322.4922, Epsilon: 0.0500\n",
            "Step: 50000, Avg Reward: 8.91, Avg Length: 17.94, Loss: 1943068.2500, Epsilon: 0.0500\n",
            "Step: 60000, Avg Reward: 9.20, Avg Length: 18.39, Loss: 33074010.0000, Epsilon: 0.0500\n",
            "Step: 70000, Avg Reward: 9.40, Avg Length: 19.08, Loss: 205881312.0000, Epsilon: 0.0500\n",
            "Step: 80000, Avg Reward: 10.63, Avg Length: 21.49, Loss: 1109689216.0000, Epsilon: 0.0500\n",
            "Step: 90000, Avg Reward: 10.38, Avg Length: 20.29, Loss: 4193675264.0000, Epsilon: 0.0500\n",
            "Step: 100000, Avg Reward: 9.55, Avg Length: 18.99, Loss: 20565917696.0000, Epsilon: 0.0500\n",
            "Step: 110000, Avg Reward: 10.98, Avg Length: 21.63, Loss: 29073852416.0000, Epsilon: 0.0500\n",
            "Step: 120000, Avg Reward: 8.54, Avg Length: 17.41, Loss: 102766051328.0000, Epsilon: 0.0500\n",
            "Step: 130000, Avg Reward: 9.54, Avg Length: 18.92, Loss: 314380353536.0000, Epsilon: 0.0500\n",
            "Step: 140000, Avg Reward: 10.81, Avg Length: 21.70, Loss: 367311519744.0000, Epsilon: 0.0500\n",
            "Step: 150000, Avg Reward: 8.99, Avg Length: 18.14, Loss: 1265543938048.0000, Epsilon: 0.0500\n",
            "Step: 160000, Avg Reward: 10.45, Avg Length: 20.64, Loss: 1807768748032.0000, Epsilon: 0.0500\n",
            "Step: 170000, Avg Reward: 10.79, Avg Length: 21.61, Loss: 3782474203136.0000, Epsilon: 0.0500\n",
            "Step: 180000, Avg Reward: 10.35, Avg Length: 20.59, Loss: 2129703993344.0000, Epsilon: 0.0500\n",
            "Step: 190000, Avg Reward: 9.98, Avg Length: 19.88, Loss: 9410424340480.0000, Epsilon: 0.0500\n",
            "Step: 200000, Avg Reward: 10.50, Avg Length: 20.96, Loss: 9752378605568.0000, Epsilon: 0.0500\n",
            "Step: 210000, Avg Reward: 8.57, Avg Length: 17.29, Loss: 16241131520000.0000, Epsilon: 0.0500\n",
            "Step: 220000, Avg Reward: 12.51, Avg Length: 25.69, Loss: 14367886147584.0000, Epsilon: 0.0500\n",
            "Step: 230000, Avg Reward: 8.16, Avg Length: 16.75, Loss: 24614889586688.0000, Epsilon: 0.0500\n",
            "Step: 240000, Avg Reward: 9.87, Avg Length: 19.68, Loss: 63339208114176.0000, Epsilon: 0.0500\n",
            "Step: 250000, Avg Reward: 10.46, Avg Length: 20.71, Loss: 59419559723008.0000, Epsilon: 0.0500\n",
            "Step: 260000, Avg Reward: 10.72, Avg Length: 21.17, Loss: 49319491141632.0000, Epsilon: 0.0500\n",
            "Step: 270000, Avg Reward: 12.58, Avg Length: 25.00, Loss: 133048205049856.0000, Epsilon: 0.0500\n",
            "Step: 280000, Avg Reward: 10.72, Avg Length: 21.77, Loss: 102256091332608.0000, Epsilon: 0.0500\n",
            "Step: 290000, Avg Reward: 10.08, Avg Length: 20.00, Loss: 173180312354816.0000, Epsilon: 0.0500\n",
            "Step: 300000, Avg Reward: 8.47, Avg Length: 17.16, Loss: 214698972676096.0000, Epsilon: 0.0500\n",
            "Step: 310000, Avg Reward: 8.56, Avg Length: 17.34, Loss: 297381018992640.0000, Epsilon: 0.0500\n",
            "Step: 320000, Avg Reward: 11.22, Avg Length: 22.50, Loss: 485538536620032.0000, Epsilon: 0.0500\n",
            "Step: 330000, Avg Reward: 10.19, Avg Length: 20.47, Loss: 492887091445760.0000, Epsilon: 0.0500\n",
            "Step: 340000, Avg Reward: 9.02, Avg Length: 17.85, Loss: 229673510371328.0000, Epsilon: 0.0500\n",
            "Step: 350000, Avg Reward: 9.45, Avg Length: 18.61, Loss: 268169570680832.0000, Epsilon: 0.0500\n",
            "Step: 360000, Avg Reward: 10.06, Avg Length: 19.91, Loss: 1199439276408832.0000, Epsilon: 0.0500\n",
            "Step: 370000, Avg Reward: 10.07, Avg Length: 20.47, Loss: 801835765989376.0000, Epsilon: 0.0500\n",
            "Step: 380000, Avg Reward: 10.13, Avg Length: 20.25, Loss: 1089903651717120.0000, Epsilon: 0.0500\n",
            "Step: 390000, Avg Reward: 10.77, Avg Length: 21.48, Loss: 416264002469888.0000, Epsilon: 0.0500\n",
            "Step: 400000, Avg Reward: 9.84, Avg Length: 19.78, Loss: 1372452236034048.0000, Epsilon: 0.0500\n",
            "Step: 410000, Avg Reward: 10.66, Avg Length: 21.58, Loss: 1483109652496384.0000, Epsilon: 0.0500\n",
            "Step: 420000, Avg Reward: 9.82, Avg Length: 19.21, Loss: 851178229334016.0000, Epsilon: 0.0500\n",
            "Step: 430000, Avg Reward: 9.48, Avg Length: 19.01, Loss: 2523088201711616.0000, Epsilon: 0.0500\n",
            "Step: 440000, Avg Reward: 10.86, Avg Length: 22.03, Loss: 1801817700696064.0000, Epsilon: 0.0500\n",
            "Step: 450000, Avg Reward: 9.54, Avg Length: 19.36, Loss: 736230274760704.0000, Epsilon: 0.0500\n",
            "Step: 460000, Avg Reward: 10.93, Avg Length: 21.47, Loss: 1230399950815232.0000, Epsilon: 0.0500\n",
            "Step: 470000, Avg Reward: 8.23, Avg Length: 16.36, Loss: 2330877409361920.0000, Epsilon: 0.0500\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8a5dbaf9e6c6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mTRAINING_START\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;31m# Update target network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8a5dbaf9e6c6>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Current Q-values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-8a5dbaf9e6c6>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         return (\n",
            "\u001b[0;32m/usr/lib/python3.11/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mselected_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/random.py\u001b[0m in \u001b[0;36m_randbelow_with_getrandbits\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mgetrandbits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrandbits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# don't use (n-1) here because n can be 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0 <= r < 2**k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE_ACTOR = 1e-4\n",
        "LEARNING_RATE_CRITIC = 1e-3\n",
        "DISCOUNT = 0.99\n",
        "TAU = 0.005  # For soft updates\n",
        "MEMORY_SIZE = 100000\n",
        "BATCH_SIZE = 64\n",
        "TOTAL_TIMESTEPS = 500000\n",
        "TRAINING_START = 10000\n",
        "NOISE_STD = 0.2\n",
        "NOISE_DECAY = 0.9995\n",
        "NOISE_MIN = 0.05\n",
        "\n",
        "# Environment parameters (adjust based on your actual environment)\n",
        "NUM_ENVS = 4\n",
        "STATE_SHAPE = (3, 48, 48)  # (C, H, W)\n",
        "ACTION_DIM = 3  # Steering, Acceleration, Brake\n",
        "ACTION_LIMITS = np.array([1.0, 1.0, 1.0])  # [steering, acceleration, brake]\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "        return (\n",
        "            torch.tensor(np.array(states), dtype=torch.float32, device=device),\n",
        "            torch.tensor(np.array(actions), dtype=torch.float32, device=device),\n",
        "            torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1),\n",
        "            torch.tensor(np.array(next_states), dtype=torch.float32, device=device),\n",
        "            torch.tensor(dones, dtype=torch.float32, device=device).unsqueeze(1),\n",
        "        )\n",
        "\n",
        "    def can_sample(self, batch_size):\n",
        "        return len(self.buffer) >= batch_size\n",
        "\n",
        "memory = ReplayBuffer(MEMORY_SIZE)\n",
        "\n",
        "# Ornstein-Uhlenbeck Noise for exploration\n",
        "class OUNoise:\n",
        "    def __init__(self, action_dim, std=0.2, theta=0.15, dt=1e-2, x0=None):\n",
        "        self.action_dim = action_dim\n",
        "        self.theta = theta\n",
        "        self.std = std\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.x = self.x0 if self.x0 is not None else np.zeros(self.action_dim)\n",
        "\n",
        "    def sample(self):\n",
        "        dx = self.theta * (-self.x) * self.dt + self.std * np.sqrt(self.dt) * np.random.normal(size=self.action_dim)\n",
        "        self.x += dx\n",
        "        return self.x\n",
        "\n",
        "noise = OUNoise(action_dim=ACTION_DIM, std=NOISE_STD)\n",
        "\n",
        "# Actor Network with Separate Scaling\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_size, action_dim, action_limits):\n",
        "        super(Actor, self).__init__()\n",
        "        self.steering_limit = torch.tensor(action_limits[0], dtype=torch.float32, device=device)\n",
        "        self.acceleration_limit = torch.tensor(action_limits[1], dtype=torch.float32, device=device)\n",
        "        self.brake_limit = torch.tensor(action_limits[2], dtype=torch.float32, device=device)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=state_size[0], out_channels=32, kernel_size=5, stride=2, padding=2),  # (32, 24, 24)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # (64, 12, 12)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # (128, 6, 6)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),  # (128, 3, 3)\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 3 * 3, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, action_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.network(x)  # Shape: (batch_size, action_dim)\n",
        "\n",
        "        # Split the output for each action component\n",
        "        steering = torch.tanh(x[:, 0]) * self.steering_limit  # [-1, 1]\n",
        "        acceleration = torch.sigmoid(x[:, 1]) * self.acceleration_limit  # [0, 1]\n",
        "        brake = torch.sigmoid(x[:, 2]) * self.brake_limit  # [0, 1]\n",
        "\n",
        "        # Combine the action components\n",
        "        return torch.stack([steering, acceleration, brake], dim=1)  # Shape: (batch_size, 3)\n",
        "\n",
        "# Critic Network\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, state_size, action_dim):\n",
        "        super(Critic, self).__init__()\n",
        "        # State pathway\n",
        "        self.state_network = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=state_size[0], out_channels=32, kernel_size=5, stride=2, padding=2),  # (32, 24, 24)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # (64, 12, 12)\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # (128, 6, 6)\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),  # (128, 3, 3)\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 3 * 3, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Action pathway\n",
        "        self.action_network = nn.Sequential(\n",
        "            nn.Linear(action_dim, 512),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        # Combined pathway\n",
        "        self.combined_network = nn.Sequential(\n",
        "            nn.Linear(512 + 512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)  # Output Q-value\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state_out = self.state_network(state)  # (batch_size, 512)\n",
        "        action_out = self.action_network(action)  # (batch_size, 512)\n",
        "        combined = torch.cat([state_out, action_out], dim=1)  # (batch_size, 1024)\n",
        "        q_value = self.combined_network(combined)  # (batch_size, 1)\n",
        "        return q_value\n",
        "\n",
        "# Initialize Actor and Critic networks\n",
        "state_size = STATE_SHAPE\n",
        "actor = Actor(state_size, ACTION_DIM, ACTION_LIMITS).to(device)\n",
        "actor_target = copy.deepcopy(actor).to(device)\n",
        "actor_target.eval()\n",
        "\n",
        "critic = Critic(state_size, ACTION_DIM).to(device)\n",
        "critic_target = copy.deepcopy(critic).to(device)\n",
        "critic_target.eval()\n",
        "\n",
        "# Optimizers\n",
        "actor_optimizer = optim.Adam(actor.parameters(), lr=LEARNING_RATE_ACTOR)\n",
        "critic_optimizer = optim.Adam(critic.parameters(), lr=LEARNING_RATE_CRITIC)\n",
        "\n",
        "# Loss function\n",
        "critic_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Soft update function\n",
        "def soft_update(target, source, tau):\n",
        "    for target_param, source_param in zip(target.parameters(), source.parameters()):\n",
        "        target_param.data.copy_(tau * source_param.data + (1.0 - tau) * target_param.data)\n",
        "\n",
        "# Training function\n",
        "def train_step():\n",
        "    if not memory.can_sample(BATCH_SIZE):\n",
        "        return None, None\n",
        "\n",
        "    states, actions, rewards, next_states, dones = memory.sample(BATCH_SIZE)\n",
        "\n",
        "    # -------------------- Critic Update -------------------- #\n",
        "    with torch.no_grad():\n",
        "        # Get target actions from target actor\n",
        "        target_actions = actor_target(next_states)\n",
        "        # Get target Q-values from target critic\n",
        "        target_qs = critic_target(next_states, target_actions)\n",
        "        # Compute y = r + gamma * Q'(s', a')\n",
        "        y = rewards + DISCOUNT * target_qs * (1 - dones)\n",
        "\n",
        "    # Get current Q-values from critic\n",
        "    current_qs = critic(states, actions)\n",
        "\n",
        "    # Compute critic loss\n",
        "    critic_loss = critic_loss_fn(current_qs, y)\n",
        "\n",
        "    # Optimize Critic\n",
        "    critic_optimizer.zero_grad()\n",
        "    critic_loss.backward()\n",
        "    # Optionally, clip gradients to prevent exploding gradients\n",
        "    torch.nn.utils.clip_grad_norm_(critic.parameters(), max_norm=1.0)\n",
        "    critic_optimizer.step()\n",
        "\n",
        "    # -------------------- Actor Update -------------------- #\n",
        "    # Get actions from current actor\n",
        "    current_actions = actor(states)\n",
        "    # Compute actor loss (maximize Q-values)\n",
        "    actor_loss = -critic(states, current_actions).mean()\n",
        "\n",
        "    # Optimize Actor\n",
        "    actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    # Optionally, clip gradients\n",
        "    torch.nn.utils.clip_grad_norm_(actor.parameters(), max_norm=1.0)\n",
        "    actor_optimizer.step()\n",
        "\n",
        "    # -------------------- Soft Update Targets -------------------- #\n",
        "    soft_update(actor_target, actor, TAU)\n",
        "    soft_update(critic_target, critic, TAU)\n",
        "\n",
        "    return critic_loss.item(), actor_loss.item()\n",
        "\n",
        "# Initialize environment (replace DummyEnv with your actual environment)\n",
        "class DummyEnv:\n",
        "    def __init__(self, num_envs, state_shape):\n",
        "        self.num_envs = num_envs\n",
        "        self.state_shape = state_shape\n",
        "\n",
        "    def reset(self):\n",
        "        # Returns (num_envs, C, H, W)\n",
        "        return np.random.randint(0, 255, size=(self.num_envs, *self.state_shape)).astype(np.uint8)\n",
        "\n",
        "    def step(self, actions):\n",
        "        # actions: numpy array of shape (num_envs, action_dim)\n",
        "        # Here, actions are already scaled to the appropriate ranges\n",
        "        next_state = np.random.randint(0, 255, size=(self.num_envs, *self.state_shape)).astype(np.uint8)\n",
        "        reward = np.random.rand(self.num_envs)\n",
        "        done = np.random.choice([False, True], size=self.num_envs, p=[0.95, 0.05])\n",
        "        info = {}\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "# Initialize environment\n",
        "env = DummyEnv(NUM_ENVS, STATE_SHAPE)\n",
        "\n",
        "# Initialize state\n",
        "state = env.reset()  # Shape: (NUM_ENVS, C, H, W)\n",
        "state = torch.tensor(state, dtype=torch.float32, device=device) / 255.0\n",
        "\n",
        "# Initialize loss variables\n",
        "critic_loss, actor_loss = None, None  # Initialize before the loop\n",
        "\n",
        "step = 0\n",
        "episode_rewards = []\n",
        "episode_lengths = []\n",
        "current_episode_reward = np.zeros(NUM_ENVS)\n",
        "current_episode_length = np.zeros(NUM_ENVS)\n",
        "\n",
        "while step < TOTAL_TIMESTEPS:\n",
        "    if step < TRAINING_START:\n",
        "        # Exploration: Select random actions within action limits\n",
        "        # Steering: [-1, 1], Acceleration: [0, 1], Brake: [0, 1]\n",
        "        steering = np.random.uniform(low=-ACTION_LIMITS[0], high=ACTION_LIMITS[0], size=(NUM_ENVS, 1))\n",
        "        acceleration = np.random.uniform(low=0, high=ACTION_LIMITS[1], size=(NUM_ENVS, 1))\n",
        "        brake = np.random.uniform(low=0, high=ACTION_LIMITS[2], size=(NUM_ENVS, 1))\n",
        "        action_vectors = np.hstack([steering, acceleration, brake])\n",
        "    else:\n",
        "        # Select actions using the actor network\n",
        "        with torch.no_grad():\n",
        "            action = actor(state).cpu().numpy()\n",
        "        # Add exploration noise\n",
        "        noise_sample = noise.sample()\n",
        "        action_vectors = action + noise_sample\n",
        "        # Clip actions to be within action limits\n",
        "        action_vectors[:,0] = np.clip(action_vectors[:,0], -ACTION_LIMITS[0], ACTION_LIMITS[0])  # Steering\n",
        "        action_vectors[:,1] = np.clip(action_vectors[:,1], 0, ACTION_LIMITS[1])  # Acceleration\n",
        "        action_vectors[:,2] = np.clip(action_vectors[:,2], 0, ACTION_LIMITS[2])  # Brake\n",
        "\n",
        "    # Execute actions in the environment\n",
        "    next_state, reward, done, _ = env.step(action_vectors)\n",
        "    next_state = torch.tensor(next_state, dtype=torch.float32, device=device) / 255.0\n",
        "\n",
        "    for i in range(NUM_ENVS):\n",
        "        memory.add((\n",
        "            state[i].cpu().numpy(),\n",
        "            action_vectors[i],\n",
        "            reward[i],\n",
        "            next_state[i].cpu().numpy(),\n",
        "            done[i]\n",
        "        ))\n",
        "        current_episode_reward[i] += reward[i]\n",
        "        current_episode_length[i] += 1\n",
        "        if done[i]:\n",
        "            episode_rewards.append(current_episode_reward[i])\n",
        "            episode_lengths.append(current_episode_length[i])\n",
        "            current_episode_reward[i] = 0\n",
        "            current_episode_length[i] = 0\n",
        "\n",
        "    state = next_state\n",
        "    step += NUM_ENVS\n",
        "\n",
        "    # Train the agent\n",
        "    if step > TRAINING_START and step % 4 == 0:\n",
        "        critic_loss, actor_loss = train_step()\n",
        "\n",
        "    # Decay noise standard deviation\n",
        "    if step > TRAINING_START:\n",
        "        noise.std = max(noise.std * NOISE_DECAY, NOISE_MIN)\n",
        "\n",
        "    # Logging\n",
        "    if step % 10000 == 0:\n",
        "        if len(episode_rewards) >= 100:\n",
        "            avg_reward = np.mean(episode_rewards[-100:])\n",
        "            avg_length = np.mean(episode_lengths[-100:])\n",
        "        else:\n",
        "            avg_reward = np.mean(episode_rewards) if len(episode_rewards) > 0 else 0\n",
        "            avg_length = np.mean(episode_lengths) if len(episode_lengths) > 0 else 0\n",
        "        last_critic_loss = critic_loss if critic_loss is not None else 0\n",
        "        last_actor_loss = actor_loss if actor_loss is not None else 0\n",
        "        print(f\"Step: {step}, Avg Reward: {avg_reward:.2f}, Avg Length: {avg_length:.2f}, Critic Loss: {last_critic_loss:.4f}, Actor Loss: {last_actor_loss:.4f}, Noise Std: {noise.std:.4f}\")\n",
        "\n",
        "    # Optionally, save the model periodically\n",
        "    if step % 100000 == 0:\n",
        "        torch.save({\n",
        "            'actor_state_dict': actor.state_dict(),\n",
        "            'critic_state_dict': critic.state_dict(),\n",
        "            'actor_target_state_dict': actor_target.state_dict(),\n",
        "            'critic_target_state_dict': critic_target.state_dict(),\n",
        "            'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
        "            'critic_optimizer_state_dict': critic_optimizer.state_dict(),\n",
        "            'noise_std': noise.std,\n",
        "        }, f\"ddpg_halminton_pytorch_step_{step}.pth\")\n",
        "\n",
        "# Save the final model\n",
        "torch.save({\n",
        "    'actor_state_dict': actor.state_dict(),\n",
        "    'critic_state_dict': critic.state_dict(),\n",
        "    'actor_target_state_dict': actor_target.state_dict(),\n",
        "    'critic_target_state_dict': critic_target.state_dict(),\n",
        "    'actor_optimizer_state_dict': actor_optimizer.state_dict(),\n",
        "    'critic_optimizer_state_dict': critic_optimizer.state_dict(),\n",
        "    'noise_std': noise.std,\n",
        "}, \"ddpg_halminton_pytorch_final.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOZg2UV2Ip_Y",
        "outputId": "dff7f145-0033-4d5b-d29b-41b62b3ced79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 10000, Avg Reward: 9.02, Avg Length: 18.26, Critic Loss: 0.0000, Actor Loss: 0.0000, Noise Std: 0.2000\n",
            "Step: 20000, Avg Reward: 11.48, Avg Length: 22.77, Critic Loss: 0.1274, Actor Loss: -2.7638, Noise Std: 0.0573\n",
            "Step: 30000, Avg Reward: 10.44, Avg Length: 21.05, Critic Loss: 0.4598, Actor Loss: -4.5441, Noise Std: 0.0500\n"
          ]
        }
      ]
    }
  ]
}